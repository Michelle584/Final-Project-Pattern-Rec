{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06704925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import warnings\n",
    "# importing os module \n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "# Imports PIL module \n",
    "from keras.models import Model\n",
    "from PIL import Image # for grabbing images\n",
    "from itertools import chain #for target labels \n",
    "from keras.utils import np_utils\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf `find -type d -name .ipynb_checkpoints` #It was automatically creating this file and causing problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a9059a-2542-4a8a-9646-9641a3850857",
   "metadata": {},
   "source": [
    "## Get Training and Test Samples as NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf8605-8c70-4c78-8e21-b61ce2159cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3360,)\n"
     ]
    }
   ],
   "source": [
    "X_train=np.load(\"train_data.npy\");\n",
    "t_train=np.load(\"train_label.npy\");\n",
    "print(t_train.shape)\n",
    "num_of_output_classes=len(np.unique(t_train))\n",
    "X_train=tensorflow.keras.applications.vgg16.preprocess_input(X_train, data_format=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ac747-564c-4591-bc02-f4f7334139a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transfer_learning_CNN(num_of_classes,save_file_as,data_train, labels_train, img_size=214, epochs=5, learning_rate=1e-4):\n",
    "    \n",
    "    # load data and labels\n",
    "    #data_train = np.load(data_train_path)\n",
    "    #labels_train_temp = np.load(labels_train_path)\n",
    "    \n",
    "    # preprocessing\n",
    "    #gray_scaled=Invert_gray_scale(data_train,300) #Converts all 300x300 images to grey scale\n",
    "    \n",
    "    #dilated_data=Dilation(gray_scaled,300) # Dilates all 300x300 images\n",
    "    \n",
    "    #bkg_removed_matrix=Remove_bkg_noise(dilated_data,300) #Removes background from 300x300 images\n",
    "    \n",
    "    #resized_matrix=resize(bkg_removed_matrix,img_size) #Resizes to img_size x img_size\n",
    " \n",
    "    # Normalize data\n",
    "    labels_train = np_utils.to_categorical(labels_train, num_of_classes)\n",
    "    \n",
    "    # specify a new input shape to replace VGG16 input layer for our data\n",
    "    new_input = (img_size, img_size, 3)\n",
    "    \n",
    "    # load model\n",
    "    model = VGG16(include_top=False, input_shape=new_input, pooling='max')\n",
    "    \n",
    "    #remove output layer to replace with new one to allow for classification of the 3 classes instead of the original VGG16\n",
    "    # classification \n",
    "    flat1 = Flatten()(model.layers[-1].output) # remove output layer\n",
    "    class1 = Dense(1024, activation='relu')(flat1) # new dense layer\n",
    "    output = Dense(num_of_classes, activation='softmax')(class1) # for 3 classes\n",
    "    \n",
    "    # define new model\n",
    "    model = Model(inputs=model.inputs, outputs=output)\n",
    "    \n",
    "    # print summary of model\n",
    "    model.summary()\n",
    "\n",
    "    # set Adam learning rate\n",
    "    adam = Adam(lr=learning_rate)\n",
    "\n",
    "    # We add metrics to get more results you want to see\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    print('Training ------------')\n",
    "    history = model.fit(data_train,labels_train, epochs=epochs)\n",
    "                \n",
    "    # plot training accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(['Training'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # save trained VGG16 CNN model\n",
    "    model.save(save_file_as)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3e1123-9631-4dc7-b54b-e45058a64948",
   "metadata": {},
   "source": [
    "## Training on all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0a56e-0fd6-46c8-b2c4-1b417a966e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train VGG16 CNN if there is no file\n",
    "if os.path.isfile('vgg16_trained_cnn.hdf5')==False:\n",
    "    train_transfer_learning_CNN(num_of_output_classes,'vgg16_trained_cnn.hdf5',X_train,t_train, img_size=100, epochs=5, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826744ef-1386-4ef2-8041-82fe059832ba",
   "metadata": {},
   "source": [
    "## Training performance when randomly pulling out 50% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a3ba6-fd88-4d1a-b19e-9c9d02740f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('vgg16_rand_pullout_50.hdf5')==False:\n",
    "    per_keep=.5 #keep 50% of data\n",
    "    X_train_rand=X_train[0:int(per_keep*X_train.shape[0]),:,:,:]\n",
    "    t_train_rand=t_train[0:int(per_keep*t_train.shape[0])]\n",
    "    print(\"New Training shape\")\n",
    "    print(t_train_rand.shape)\n",
    "    print(X_train_rand.shape)\n",
    "    train_transfer_learning_CNN(num_of_output_classes,'vgg16_rand_pullout_50.hdf5',X_train_rand,t_train_rand, img_size=100, epochs=5, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f13290-bb2e-4828-a845-f178720626b4",
   "metadata": {},
   "source": [
    "## Training performance when randomly pulling out 80% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfc388-1f8c-40a9-9bc3-ea32dd547ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('vgg16_rand_pullout_80.hdf5')==False:\n",
    "    per_keep=.2 #keep 20% of data\n",
    "    X_train_rand=X_train[0:int(per_keep*X_train.shape[0]),:,:,:]\n",
    "    t_train_rand=t_train[0:int(per_keep*t_train.shape[0])]\n",
    "    print(\"New Training shape\")\n",
    "    print(t_train_rand.shape)\n",
    "    print(X_train_rand.shape)\n",
    "\n",
    "    train_transfer_learning_CNN(num_of_output_classes,'vgg16_rand_pullout_80.hdf5',X_train_rand,t_train_rand, img_size=100, epochs=5, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b224fa0a-dd20-4f61-becf-b0715f97ecee",
   "metadata": {},
   "source": [
    "## Using Clustering to Take Out \"Redundant\" Images for Higher Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe4a6e-5064-49aa-986c-551fb0ffeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train/255.0\n",
    "#reshaped_data = X_train.reshape(len(X_train),-1)\n",
    "#print(reshaped_data.shape)\n",
    "#kmeans = KMeans(n_clusters=, random_state=0)\n",
    "#clusters = kmeans.fit_predict(reshaped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d8156e0-219d-4c0e-9867-f0cc85d64de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3360, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Tommorow do a clustering on RGB values\n",
    "num_pixels=100;\n",
    "Processed_images=np.zeros((X_train.shape[0],num_pixels,num_pixels))\n",
    "for i in range(X_train.shape[0]):\n",
    "    img_gray = cv2.cvtColor(X_train[i,:,:,:], cv2.COLOR_RGB2GRAY)\n",
    "    #image=Image.fromarray(img_gray)\n",
    "    #edges = cv2.Canny(image, threshold1=30, threshold2=100)\n",
    "    img_gray=img_gray_gray.reshape(20,20)\n",
    "    Processed_images[i,:,:]=abs(img_gray);\n",
    "    \n",
    "Processed_images=Processed_images.reshape(X_train.shape[0],64)\n",
    "print(Processed_images.shape)\n",
    "means=np.zeros((12,64)); # Mean of 10 classes with 64 pixel(features)\n",
    "variance=np.zeros((12,20,));\n",
    "variance_vals_array=np.zeros((12,1))\n",
    "for i in range(12):\n",
    "    locations=np.where(t_train == i)\n",
    "    data_length_per_class=Processed_images[locations].shape[0]\n",
    "    means[i,:]=np.sum(Processed_images[locations],axis=0)/data_length_per_class\n",
    "    variance[i,:,:]=np.cov(Processed_images[locations].T)\n",
    "means=np.transpose(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ce28a26-537f-42eb-8cf1-cbcdec22f044",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m prior_per_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m; \u001b[38;5;66;03m# initialize\u001b[39;00m\n\u001b[1;32m      5\u001b[0m prior_per_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msum\u001b[39m(t_train\u001b[38;5;241m==\u001b[39mj)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(t_train);\n\u001b[0;32m----> 6\u001b[0m rv \u001b[38;5;241m=\u001b[39m \u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#print(rv.pdf(X_test[i,:])*prior_per_class)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m posterior[i,j]\u001b[38;5;241m=\u001b[39mrv\u001b[38;5;241m.\u001b[39mpdf(Processed_images[i,:])\u001b[38;5;241m*\u001b[39mprior_per_class\n",
      "File \u001b[0;32m/apps/tensorflow/2.7.0/lib/python3.9/site-packages/scipy/stats/_multivariate.py:360\u001b[0m, in \u001b[0;36mmultivariate_normal_gen.__call__\u001b[0;34m(self, mean, cov, allow_singular, seed)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cov\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, allow_singular\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;124;03m\"\"\"Create a frozen multivariate normal distribution.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    See `multivariate_normal_frozen` for more information.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultivariate_normal_frozen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_singular\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/apps/tensorflow/2.7.0/lib/python3.9/site-packages/scipy/stats/_multivariate.py:730\u001b[0m, in \u001b[0;36mmultivariate_normal_frozen.__init__\u001b[0;34m(self, mean, cov, allow_singular, seed, maxpts, abseps, releps)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dist \u001b[38;5;241m=\u001b[39m multivariate_normal_gen(seed)\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dist\u001b[38;5;241m.\u001b[39m_process_parameters(\n\u001b[1;32m    729\u001b[0m                                                     \u001b[38;5;28;01mNone\u001b[39;00m, mean, cov)\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcov_info \u001b[38;5;241m=\u001b[39m \u001b[43m_PSD\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_singular\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m maxpts:\n\u001b[1;32m    732\u001b[0m     maxpts \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000000\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim\n",
      "File \u001b[0;32m/apps/tensorflow/2.7.0/lib/python3.9/site-packages/scipy/stats/_multivariate.py:158\u001b[0m, in \u001b[0;36m_PSD.__init__\u001b[0;34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, M, cond\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rcond\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m              check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_singular\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Compute the symmetric eigendecomposition.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Note that eigh takes care of array conversion, chkfinite,\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# and assertion that the matrix is square.\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     s, u \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     eps \u001b[38;5;241m=\u001b[39m _eigvalsh_to_eps(s, cond, rcond)\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(s) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39meps:\n",
      "File \u001b[0;32m/apps/tensorflow/2.7.0/lib/python3.9/site-packages/scipy/linalg/_decomp.py:547\u001b[0m, in \u001b[0;36meigh\u001b[0;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite, subset_by_index, subset_by_value, driver)\u001b[0m\n\u001b[1;32m    544\u001b[0m         lwork_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlwork\u001b[39m\u001b[38;5;124m'\u001b[39m: lw}\n\u001b[1;32m    546\u001b[0m     drv_args\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m: lower, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompute_v\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _job \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m})\n\u001b[0;32m--> 547\u001b[0m     w, v, \u001b[38;5;241m*\u001b[39mother_args, info \u001b[38;5;241m=\u001b[39m \u001b[43mdrv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdrv_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlwork_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Generalized problem\u001b[39;00m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;66;03m# 'gvd' doesn't have lwork query\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m driver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgvd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "posterior=np.zeros((len(X_train),12));\n",
    "for i in range(len(X_train)):\n",
    "     for j in range(12):\n",
    "        prior_per_class=0; # initialize\n",
    "        prior_per_class=sum(t_train==j)/len(t_train);\n",
    "        rv = multivariate_normal(means[:,j], variance[j,:,:],allow_singular=True)\n",
    "        #print(rv.pdf(X_test[i,:])*prior_per_class)\n",
    "        posterior[i,j]=rv.pdf(Processed_images[i,:])*prior_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23fb80c-1f3d-4ba3-af56-96645f2133aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations=np.zeros((len(X_train)))\n",
    "for i in range(len(X_test)):\n",
    "    max1 = 0\n",
    "    for j in range(10):\n",
    "        if posterior[i][j] > max1 :\n",
    "                max1 =posterior[i,j]\n",
    "                location=j\n",
    "    locations[i]=location\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy_score(t_train,locations))\n",
    "print(confusion_matrix(t_train, locations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.7.0",
   "language": "python",
   "name": "tensorflow-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
